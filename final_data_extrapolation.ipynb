{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "original_data=pd.read_csv('D:\\\\student\\\\merged.csv')\n",
    "original_cols=list(original_data.columns)\n",
    "mcols=[]\n",
    "cls=['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'nursery', 'internet', 'guardian.y', 'traveltime.y', 'studytime.y', 'failures.y', 'schoolsup.y', 'famsup.y', 'paid.y', 'activities.y', 'higher.y', 'romantic.y', 'famrel.y', 'freetime.y', 'goout.y', 'Dalc.y', 'Walc.y', 'health.y', 'absences.y', 'G1.y', 'G2.y', 'G3.y']\n",
    "for col in original_cols[1:]:\n",
    "    if col == 'guardian.y':\n",
    "        break\n",
    "    mcols.append(col)\n",
    "pcols=[c for c in original_cols if c in cls]# portugese subject columns\n",
    "maths_subject_data=original_data[mcols]\n",
    "portugese_subject_data=original_data[pcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the following code in convert_categories is used for converting nominal category features like school,sex etc \\n   to numeric categories'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"the following code in convert_categories is used for converting nominal category features like school,sex etc \n",
    "   to numeric categories\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def convert_categories(datfr):#convert the nominal cateogries to numerical values\n",
    "    dcols=list(datfr.columns)\n",
    "    for col in dcols:\n",
    "        if datfr[col].dtype=='object':#if the column has nominal categorical data\n",
    "            datfr[col]=datfr[col].astype('category')\n",
    "            datfr[col]=datfr[col].cat.codes\n",
    "    return datfr\n",
    "maths_processed=convert_categories(maths_subject_data)\n",
    "portugese_processed=convert_categories(portugese_subject_data)\n",
    "existing_rows=maths_processed.shape[0]#number of students in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the code in the following cell takes a set of features and then generate a bigger feature dataset based on distribution \\nof existing values in each column.\\n calculate_percentages(column)-> calculates the percentages of different unique values in the column\\nget_extra_data()-> based on percentages calculated for different values in a column they are added randomly to form \\na bigger dataset'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"the code in the following cell takes a set of features and then generate a bigger feature dataset based on distribution \n",
    "of existing values in each column.\n",
    " calculate_percentages(column)-> calculates the percentages of different unique values in the column\n",
    "get_extra_data()-> based on percentages calculated for different values in a column they are added randomly to form \n",
    "a bigger dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "#random data generation based on existing distribution\n",
    "def calculate_percentages(column):#cl-> column\n",
    "    column_data=list(column)#create a list\n",
    "    counts_pattern=Counter(column_data)\n",
    "    total_count=len(column_data)\n",
    "    percentages={}\n",
    "    for element,count in counts_pattern.items():\n",
    "        if element not in percentages.keys():\n",
    "            percentages[element]=round(count/total_count,3)#calculate the percentage of no of times that particular data item has appeared\n",
    "    return percentages\n",
    "def get_extra_data(per,number):#create synthetic data based on existing distribution\n",
    "    possible={}\n",
    "    for item,p in per.items():\n",
    "        if item not in possible.keys():\n",
    "            occur=round(p*number,0)\n",
    "            possible[item]=int(occur)\n",
    "    total_count=sum(possible.values())\n",
    "    high=max(possible.values())\n",
    "    low=min(possible.values())\n",
    "    print(total_count)\n",
    "    if total_count>number:#if the total count exceeds the possibility then subtract the diff from highest occurring element\n",
    "        #print('sudheer')\n",
    "        diff=total_count-number\n",
    "        for element,count in possible.items():\n",
    "            if count==high:\n",
    "                possible[element]=count-diff\n",
    "                break\n",
    "    if total_count<number:#if the total count is short of possibility then add the diff to the one of the lowest occurring element\n",
    "        #print('kumar')\n",
    "        diff=number-total_count\n",
    "        for element,count in possible.items():\n",
    "            if count==low:\n",
    "                possible[element]=count+diff\n",
    "                break\n",
    "                \n",
    "    synthetic_data=[]\n",
    "    for item,occur in possible.items():\n",
    "        for i in range(occur):\n",
    "            synthetic_data.append(item)\n",
    "    print(len(synthetic_data))\n",
    "    jumbled_synthetic_data=[]\n",
    "    tracking_data=[]\n",
    "    synthetic_indices=list(range(len(synthetic_data)))\n",
    "    while True:#jumbling the synthetic data list\n",
    "        a=random.choice(synthetic_indices)\n",
    "        if a not in tracking_data:\n",
    "            data=synthetic_data[a]\n",
    "            jumbled_synthetic_data.append(data)\n",
    "        if len(jumbled_synthetic_data)==len(synthetic_data):\n",
    "            break\n",
    "    return jumbled_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate synthetic data function calls the calculate percentages and get extra data functions \\nand gathers a list of list consisting of original data and synthetic data .Then a dataframe is created for \\nthe bigger dataset'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"generate synthetic data function calls the calculate percentages and get extra data functions \n",
    "and gathers a list of list consisting of original data and synthetic data .Then a dataframe is created for \n",
    "the bigger dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate synthetic data for all columns and store that data in a dataframe\n",
    "def generate_synthetic_data(additional,df,cols):\n",
    "    final_data=[]\n",
    "    expanded_data=[]\n",
    "    for i in cols:#only features\n",
    "        dt=df[i]\n",
    "        actual=list(dt)\n",
    "        print('...............................')\n",
    "        percents=calculate_percentages(dt)\n",
    "        extra=get_extra_data(percents,additional)\n",
    "        actual.extend(extra)\n",
    "        final_data.append(actual)\n",
    "        print('**********************************************') \n",
    "    expanded_length=existing_rows+additional\n",
    "    for i in range(expanded_length):#re-arranging list of lists to create dataframe\n",
    "        t=[]\n",
    "        for j in range(len(final_data)):\n",
    "            t.append(final_data[j][i])\n",
    "        expanded_data.append(t)\n",
    "    print(len(expanded_data))\n",
    "    bigger_data=pd.DataFrame(expanded_data,columns=cols)\n",
    "    return bigger_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the following code fits linear regression over original feature data and response variables and then it would predict\\nthe marks for synthetic data thereby it reduces the randomness in the synthetic data and establishes some \\nrelationship  which would make the additional synthetic data valuable in improving the fit of the model\\nThis procedure is implemented for both maths and portugese subjects separately .Later the two dataframes are joinned together\\nThe synthetic data for demographic features is generated only once since these features are common in both subjects'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" the following code fits linear regression over original feature data and response variables and then it would predict\n",
    "the marks for synthetic data thereby it reduces the randomness in the synthetic data and establishes some \n",
    "relationship  which would make the additional synthetic data valuable in improving the fit of the model\n",
    "This procedure is implemented for both maths and portugese subjects separately .Later the two dataframes are joinned together\n",
    "The synthetic data for demographic features is generated only once since these features are common in both subjects\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter the additional number of students you would like to addd to the exisiting dataset: 500\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "502\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "508\n",
      "500\n",
      "**********************************************\n",
      "882\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "500\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "499\n",
      "500\n",
      "**********************************************\n",
      "...............................\n",
      "501\n",
      "500\n",
      "**********************************************\n",
      "882\n",
      "generated 'G1.x' marks based on synthetic data\n",
      "generated 'G2.x' marks based on synthetic data\n",
      "generated 'G3.x' marks based on synthetic data\n",
      "generated 'G1.y' marks of portugese subject based on synthetic data\n",
      "generated 'G2.y' marks of portugese subject based on synthetic data\n",
      "generated 'G3.y' marks of portugese subject based on synthetic data\n",
      "(882, 33) (882, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\sudhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "more_students=int(input('please enter the additional number of students you would like to addd to the exisiting dataset: '))\n",
    "labels=['G1.x','G2.x','G3.x']#maths subject marks columns\n",
    "port_labels=['G1.y','G2.y','G3.y']#portugese subject marks columns\n",
    "feature_cols=mcols[:-3]#excludes all three marks columns i.e only features of maths subject\n",
    "port_feature_cols=pcols[:-3]#feature columns for portugese subject\n",
    "feature_data=maths_processed[feature_cols]#maths subject feature data\n",
    "port_feature_data=portugese_processed[port_feature_cols]#portugese subject features\n",
    "big_data=generate_synthetic_data(more_students,feature_data,feature_cols)#get bigger dataset for maths subject \n",
    "#extract the demographic data from maths bigger dataset since they are common to both\n",
    "pdemo_data=big_data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus','Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'nursery', 'internet']]#demographic data for portugese subject\n",
    "#generate synthetic data for portugese subject\n",
    "pdata=generate_synthetic_data(more_students,port_feature_data,port_feature_cols)#portugese features synthetic data\n",
    "#filtering only school-related features data for portugese subject\n",
    "pschool_data=pdata[['guardian.y','traveltime.y', 'studytime.y', 'failures.y', 'schoolsup.y', 'famsup.y','paid.y', 'activities.y', 'higher.y', 'romantic.y', 'famrel.y','freetime.y', 'goout.y', 'Dalc.y', 'Walc.y', 'health.y', 'absences.y']]\n",
    "portugese_big_data=pdemo_data.join(pschool_data)#bigger dataset for portugese subject\n",
    "#creating a dataframe\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "for lab in labels:\n",
    "    label_data=maths_processed[lab]\n",
    "    #lnr=LinearRegression()\n",
    "    #lnr=DecisionTreeRegressor()\n",
    "    lnr=RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    lnr.fit(feature_data,label_data)#fitting a linear regression model on small dataset\n",
    "    necc_data=big_data[382:]#synthetic data\n",
    "    predicted=lnr.predict(necc_data)#predicting only on synthetic data\n",
    "    proper_predicted=[int(round(i,0)) for i in predicted]#convert float to int along with round off numbers \n",
    "    orig_marks=list(label_data)\n",
    "    feature_data[lab]=orig_marks\n",
    "    orig_marks.extend(proper_predicted)\n",
    "    big_data[lab]=orig_marks\n",
    "    print('generated %r marks based on synthetic data' % lab)\n",
    "for lab in port_labels:\n",
    "    label_data=portugese_processed[lab]\n",
    "    lnr=LinearRegression()\n",
    "    lnr.fit(port_feature_data,label_data)#fitting a linear regression model on small dataset\n",
    "    necc_data=portugese_big_data[382:]#synthetic data\n",
    "    predicted=lnr.predict(necc_data)#predicting only on synthetic data\n",
    "    proper_predicted=[int(round(i,0)) for i in predicted]#convert float to int along with round off numbers \n",
    "    orig_marks=list(label_data)\n",
    "    port_feature_data[lab]=orig_marks\n",
    "    orig_marks.extend(proper_predicted)\n",
    "    portugese_big_data[lab]=orig_marks\n",
    "    print('generated %r marks of portugese subject based on synthetic data' % lab)\n",
    "    \n",
    "print(big_data.shape,portugese_big_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both maths and portugese dataframes and saved it to the csv file to form combined bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "portu_unique=['guardian.y','traveltime.y', 'studytime.y', 'failures.y', 'schoolsup.y', 'famsup.y','paid.y', 'activities.y', 'higher.y', 'romantic.y', 'famrel.y','freetime.y', 'goout.y', 'Dalc.y', 'Walc.y', 'health.y', 'absences.y',\n",
    "       'G1.y', 'G2.y', 'G3.y']\n",
    "comp_data=big_data.join(portugese_big_data[portu_unique])\n",
    "comp_data.to_csv('D:\\\\student\\\\final_combined_daatset.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
